version: '3'
networks:
  default:
    name: airflow-network
services:
    postgres:
        build: ./docker/postgres
        environment:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        volumes:
          - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
          test: ["CMD", "pg_isready", "-U", "airflow"]
          interval: 5s
          retries: 5
        restart: always
    redis:
        image: redis:latest
        expose:
          - 6379
        healthcheck:
          test: ["CMD", "redis-cli", "ping"]
          interval: 5s
          timeout: 30s
          retries: 50
        restart: always
    adminer:
      image: wodby/adminer:latest
      restart: always
      container_name: adminer
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      ports:
        - "32767:9000"
      environment:
        - ADMINER_DEFAULT_DB_DRIVER=psql
        - ADMINER_DEFAULT_DB_HOST=postgres
        - ADMINER_DEFAULT_DB_NAME=airflow
      healthcheck:
        test: [ "CMD", "nc", "-z", "adminer", "9000" ]
        timeout: 45s
        interval: 10s
        retries: 10
######################################################
# HADOOP SERVICES
######################################################
    namenode:
      build: ./docker/hadoop/hadoop-namenode
      restart: always
      container_name: namenode
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      ports:
        - "32763:9870"
      volumes:
        - ./mnt/hadoop/namenode:/hadoop/dfs/name
      environment:
        - CLUSTER_NAME=hadoop_cluster
      healthcheck:
        test: [ "CMD", "nc", "-z", "namenode", "9870" ]
        timeout: 45s
        interval: 10s
        retries: 10

    datanode:
      build: ./docker/hadoop/hadoop-datanode
      restart: always
      container_name: datanode
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      depends_on:
        - namenode
      volumes:
        - ./mnt/hadoop/datanode:/hadoop/dfs/data
      environment:
        - SERVICE_PRECONDITION=namenode:9870
      healthcheck:
        test: [ "CMD", "nc", "-z", "datanode", "9864" ]
        timeout: 45s
        interval: 10s
        retries: 10

    hive-metastore:
      build: ./docker/hive/hive-metastore
      restart: always
      container_name: hive-metastore
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      depends_on:
        - namenode
        - datanode
        - postgres
      environment:
        - SERVICE_PRECONDITION=namenode:9870 datanode:9864 postgres:5432
      ports:
        - "32761:9083"
      healthcheck:
        test: [ "CMD", "nc", "-z", "hive-metastore", "9083" ]
        timeout: 45s
        interval: 10s
        retries: 10

    hive-server:
      build: ./docker/hive/hive-server
      restart: always
      container_name: hive-server
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      depends_on:
        - hive-metastore
      environment:
        - SERVICE_PRECONDITION=hive-metastore:9083
      ports:
        - "32760:10000"
        - "32759:10002"
      healthcheck:
        test: [ "CMD", "nc", "-z", "hive-server", "10002" ]
        timeout: 45s
        interval: 10s
        retries: 10

    hive-webhcat:
      build: ./docker/hive/hive-webhcat
      restart: always
      container_name: hive-webhcat
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      depends_on:
        - hive-server
      environment:
        - SERVICE_PRECONDITION=hive-server:10000
      healthcheck:
        test: [ "CMD", "nc", "-z", "hive-webhcat", "50111" ]
        timeout: 45s
        interval: 10s
        retries: 10

    hue:
      build: ./docker/hue
      restart: always
      container_name: hue
      logging:
        driver: "json-file"
        options:
            max-file: "5"
            max-size: "10m"
      depends_on:
        - hive-server
        - postgres
      ports:
        - "32762:8888"
      volumes:
        - ./mnt/hue/hue.ini:/usr/share/hue/desktop/conf/z-hue.ini
      environment:
        - SERVICE_PRECONDITION=hive-server:10000 postgres:5432
      healthcheck:
        test: [ "CMD", "nc", "-z", "hue", "8888" ]
        timeout: 45s
        interval: 10s
        retries: 10

    airflow-webserver:
        build:
            context: ./docker/airflow
        image: apache/airflow:2.5.1
        healthcheck:
          test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
          interval: 10s
          timeout: 10s
          retries: 5
        restart: always
        depends_on:
            - postgres
            - redis
        environment:
            - EXECUTOR=CeleryExecutor
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - PYTHONPATH=/usr/local/airflow/conf/
        volumes:
            - ./mnt/airflow/dags:/usr/local/airflow/dags
            - ./mnt/airflow/airflow.cfg:/opt/airflow/airflow.cfg
            - ./mnt/airflow/logs:/usr/local/airflow/logs
            - ./mnt/airflow/conf:/usr/local/airflow/conf
            # Uncomment to include custom plugins
            # - ./plugins:/usr/local/airflow/plugins
        ports:
            - "8080:8080"
        command: webserver

    flower:
        build:
            context: ./docker/airflow
        image: apache/airflow:2.5.1
        restart: always
        depends_on:
            - redis
        environment:
            - EXECUTOR=CeleryExecutor
            - REDIS_PASSWORD=redispass
        ports:
            - "5555:5555"
        command: celery flower

    airflow-scheduler:
        build:
            context: ./docker/airflow
        image: apache/airflow:2.5.1
        restart: always
        depends_on:
            - airflow-webserver
        volumes:
            - ./mnt/airflow/dags:/usr/local/airflow/dags
            - ./mnt/airflow/airflow.cfg:/opt/airflow/airflow.cfg
            - ./mnt/airflow/logs:/usr/local/airflow/logs   
            - ./mnt/airflow/conf:/usr/local/airflow/conf
        # Uncomment to include custom plugins
            # - ./plugins:/usr/local/airflow/plugins
        environment:
            - EXECUTOR=CeleryExecutor
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - PYTHONPATH=/usr/local/airflow/conf/
        command: scheduler
        healthcheck:
          test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
          interval: 10s
          timeout: 10s
          retries: 5
    worker:
        build:
            context: ./docker/airflow
        image: apache/airflow:2.5.1
        restart: always
        depends_on:
            - airflow-scheduler
        volumes:
            - ./mnt/airflow/dags:/usr/local/airflow/dags
            - ./mnt/airflow/airflow.cfg:/opt/airflow/airflow.cfg
            - ./mnt/airflow/logs:/usr/local/airflow/logs   
            - ./mnt/airflow/conf:/usr/local/airflow/conf  
            # Uncomment to include custom plugins
            # - ./plugins:/usr/local/airflow/plugins
        environment:
            - EXECUTOR=CeleryExecutor
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - PYTHONPATH=/usr/local/airflow/conf/
        command: celery worker
        healthcheck:
          test:
            - "CMD-SHELL"
            - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
          interval: 10s
          timeout: 10s
          retries: 5
    airflow-cli:
        build:
            context: ./docker/airflow
        image: apache/airflow:2.5.1
        profiles:
          - debug
        environment:
            - EXECUTOR=CeleryExecutor
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
            - PYTHONPATH=/usr/local/airflow/conf/
        # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
        command:
          - bash
          - -c
          - airflow
    # airflow-init:
    #     build:
    #         context: ./docker/airflow
    #     image: apache/airflow:2.5.1
    #     entrypoint: /bin/bash
    #     # yamllint disable rule:line-length
    #     command:
    #       - -c
    #       - |
    #         function ver() {
    #           printf "%04d%04d%04d%04d" $${1//./ }
    #         }
    #         airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && gosu airflow airflow version)
    #         airflow_version_comparable=$$(ver $${airflow_version})
    #         min_airflow_version=2.2.0
    #         min_airflow_version_comparable=$$(ver $${min_airflow_version})
    #         if (( airflow_version_comparable < min_airflow_version_comparable )); then
    #           echo
    #           echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
    #           echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
    #           echo
    #           exit 1
    #         fi
    #         if [[ -z "${AIRFLOW_UID}" ]]; then
    #           echo
    #           echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
    #           echo "If you are on Linux, you SHOULD follow the instructions below to set "
    #           echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
    #           echo "For other operating systems you can get rid of the warning with manually created .env file:"
    #           echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
    #           echo
    #         fi
    #         one_meg=1048576
    #         mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
    #         cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
    #         disk_available=$$(df / | tail -1 | awk '{print $$4}')
    #         warning_resources="false"
    #         if (( mem_available < 4000 )) ; then
    #           echo
    #           echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
    #           echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
    #           echo
    #           warning_resources="true"
    #         fi
    #         if (( cpus_available < 2 )); then
    #           echo
    #           echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
    #           echo "At least 2 CPUs recommended. You have $${cpus_available}"
    #           echo
    #           warning_resources="true"
    #         fi
    #         if (( disk_available < one_meg * 10 )); then
    #           echo
    #           echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
    #           echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
    #           echo
    #           warning_resources="true"
    #         fi
    #         if [[ $${warning_resources} == "true" ]]; then
    #           echo
    #           echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
    #           echo "Please follow the instructions to increase amount of resources available:"
    #           echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
    #           echo
    #         fi
    #         exec /entrypoint airflow version
    #     # yamllint enable rule:line-length
    #     environment:
    #       _AIRFLOW_DB_UPGRADE: 'true'
    #       _AIRFLOW_WWW_USER_CREATE: 'true'
    #       _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
    #       _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    #       _PIP_ADDITIONAL_REQUIREMENTS: ''
    #     user: "0:0"
    #     volumes:
    #         - ${AIRFLOW_PROJ_DIR:-.}:/sources
volumes:
  postgres-db-volume: