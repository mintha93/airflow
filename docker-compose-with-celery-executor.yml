version: '3.9'
networks:
  airflow:

services:
  postgres:
    image: postgres:13.1
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_DB=airflow
      - POSTGRES_PASSWORD=airflow
      - PGDATA=/var/lib/postgresql/data/pgdata
    ports:
      - 5432:5432
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./database/data:/var/lib/postgresql/data/pgdata
      - ./database/logs:/var/lib/postgresql/data/log
    command: >
     postgres
       -c listen_addresses=*
       -c logging_collector=on
       -c log_destination=stderr
       -c max_connections=200
    networks:
      - airflow
  redis:
    image: redis:5.0.5
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379
    ports:
      - 6379:6379
    networks:
      - airflow
  webserver:
    build:
        context: ./docker/airflow
    env_file:
      - .env
    image: apache/airflow:2.5.0-python3.8
    ports:
      - 8080:8080
    volumes:
      - ./mnt/airflow/dags:/opt/airflow/dags
      - ./mnt/airflow/logs:/opt/airflow/logs
      - ./mnt/airflow/files:/opt/airflow/files
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    depends_on:
      - postgres
      - redis
      - initdb
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
    networks:
      - airflow
  flower:
    build:
        context: ./docker/airflow
    image: apache/airflow:2.5.0-python3.8
    env_file:
      - .env
    ports:
      - 5555:5555
    depends_on:
      - redis
    deploy:
      restart_policy:
        condition: on-failure
        delay: 8s
        max_attempts: 3
    volumes:
      - ./mnt/airflow/logs:/opt/airflow/logs
    command: celery flower
    networks:
      - airflow
  scheduler:
    build:
        context: ./docker/airflow
    image: apache/airflow:2.5.0-python3.8
    env_file:
      - .env
    volumes:
      - ./mnt/airflow/dags:/opt/airflow/dags
      - ./mnt/airflow/logs:/opt/airflow/logs
      - ./mnt/airflow/files:/opt/airflow/files
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    depends_on:
      - initdb
    deploy:
      restart_policy:
        condition: always
        delay: 5s
        window: 120s
    networks:
      - airflow
  worker_1:
    build:
        context: ./docker/airflow
    image: apache/airflow:2.5.0-python3.8
    env_file:
      - .env
    volumes:
      - ./mnt/airflow/dags:/opt/airflow/dags
      - ./mnt/airflow/logs:/opt/airflow/logs
      - ./mnt/airflow/files:/opt/airflow/files
      - /var/run/docker.sock:/var/run/docker.sock
    command: celery worker -H worker_1_name
    depends_on:
      - scheduler
    deploy:
      restart_policy:
        condition: on-failure
        delay: 8s
        max_attempts: 3
    networks:
      - airflow
  worker_2:
    build:
        context: ./docker/airflow
    image: apache/airflow:2.5.0-python3.8
    env_file:
      - .env
    volumes:
      - ./mnt/airflow/dags:/opt/airflow/dags
      - ./mnt/airflow/logs:/opt/airflow/logs
      - ./mnt/airflow/files:/opt/airflow/files
      - /var/run/docker.sock:/var/run/docker.sock
    command: celery worker -H worker_2_name
    depends_on:
      - scheduler
    deploy:
      restart_policy:
        condition: on-failure
        delay: 8s
        max_attempts: 3
    networks:
      - airflow
  initdb:
    image: apache/airflow:2.5.0-python3.8    
    env_file:
      - .env
    volumes:
      - ./mnt/airflow/dags:/opt/airflow/dags
      - ./mnt/airflow/logs:/opt/airflow/logs
      - ./mnt/airflow/files:/opt/airflow/files
      - /var/run/docker.sock:/var/run/docker.sock
    entrypoint: /bin/bash
    deploy:
      restart_policy:
        condition: on-failure
        delay: 8s
        max_attempts: 5
    command: -c "airflow db init && airflow users create --firstname admin --lastname admin --email admin --password admin --username admin --role Admin"
    depends_on:
      - redis
      - postgres
    networks:
      - airflow
######################################################
# SPARK SERVICES
######################################################
  spark-master:
    image: bitnami/spark:3.2.1
    restart: always
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    ports:
      - "32766:8082"
      - "32765:7077"
    volumes:
      - ./mnt/spark/apps:/opt/spark-apps
      - ./mnt/spark/data:/opt/spark-data
    healthcheck:
      test: [ "CMD", "nc", "-z", "spark-master", "8082" ]
      timeout: 45s
      interval: 10s
      retries: 10
    networks:
       - airflow
  spark-worker:
    image: bitnami/spark:3.2.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    depends_on:
      - spark-master
    ports:
      - "32764:8081"
    volumes:
      - ./mnt/spark/apps:/opt/spark-apps
      - ./mnt/spark/data:/opt/spark-data
    healthcheck:
      test: [ "CMD", "nc", "-z", "spark-worker", "8081" ]
      timeout: 45s
      interval: 10s
      retries: 10
    networks:
       - airflow
  # jupyter-pyspark:
  #   image: jupyter/pyspark-notebook:spark-3.2.1 # pyspark
  #   #image: jupyter/all-spark-notebook:spark-3.2.1 # scala
  #   #image: jupyter/datascience-notebook:latest # julia 
  #   ports:
  #     - "8888:8888"
  #   volumes:
  #     - ./mnt/notebooks:/opt/notebooks/
  #   networks:
  #      - airflow
  # livy:
  #    build: ./docker/livy
  #    restart: always
  #    container_name: livy
  #    logging:
  #      driver: "json-file"
  #      options:
  #          max-file: "5"
  #          max-size: "10m"
  #    depends_on:
  #      - spark-worker
  #    ports:
  #      - "32758:8998"
  #    environment:
  #      - SPARK_MASTER_ENDPOINT=spark-master
  #      - SPARK_MASTER_PORT=7077
  #      - DEPLOY_MODE=client
  #    healthcheck:
  #      test: [ "CMD", "nc", "-z", "livy", "8998" ]
  #      timeout: 45s
  #      interval: 10s
  #      retries: 10
######################################################
# HADOOP SERVICES
######################################################
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    restart: always
    container_name: namenode
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    ports:
      - "32763:9870"
    volumes:
      - ./mnt/hadoop/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=hadoop_cluster
    healthcheck:
      test: [ "CMD", "nc", "-z", "namenode", "9870" ]
      timeout: 45s
      interval: 10s
      retries: 10
    networks:
       - airflow
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    restart: always
    container_name: datanode
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    depends_on:
      - namenode
    volumes:
      - ./mnt/hadoop/datanode:/hadoop/dfs/data
    environment:
      - SERVICE_PRECONDITION=namenode:9870
    healthcheck:
      test: [ "CMD", "nc", "-z", "datanode", "9864" ]
      timeout: 45s
      interval: 10s
      retries: 10
    networks:
       - airflow
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    restart: always
    container_name: hive-metastore
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    depends_on:
      - namenode
      - datanode
      - postgres
    environment:
      - SERVICE_PRECONDITION=namenode:9870 datanode:9864 postgres:5432
    ports:
      - "32761:9083"
    healthcheck:
      test: [ "CMD", "nc", "-z", "hive-metastore", "9083" ]
      timeout: 45s
      interval: 10s
      retries: 10
    networks:
       - airflow
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    restart: always
    container_name: hive-server
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    depends_on:
      - hive-metastore
    environment:
      - SERVICE_PRECONDITION=hive-metastore:9083
    ports:
      - "32760:10000"
      - "32759:10002"
    healthcheck:
      test: [ "CMD", "nc", "-z", "hive-server", "10002" ]
      timeout: 45s
      interval: 10s
      retries: 10

  # hive-webhcat:
  #   build: ./docker/hive/hive-webhcat
  #   restart: always
  #   container_name: hive-webhcat
  #   logging:
  #     driver: "json-file"
  #     options:
  #         max-file: "5"
  #         max-size: "10m"
  #   depends_on:
  #     - hive-server
  #   environment:
  #     - SERVICE_PRECONDITION=hive-server:10000
  #   healthcheck:
  #     test: [ "CMD", "nc", "-z", "hive-webhcat", "50111" ]
  #     timeout: 45s
  #     interval: 10s
  #     retries: 10

  hue:
    image: bde2020/hdfs-filebrowser:3.11
    restart: always
    container_name: hue
    logging:
      driver: "json-file"
      options:
          max-file: "5"
          max-size: "10m"
    depends_on:
      - hive-server
      - postgres
    ports:
      - "32762:8888"
    volumes:
      - ./mnt/hue/hue.ini:/usr/share/hue/desktop/conf/z-hue.ini
    environment:
      - SERVICE_PRECONDITION=hive-server:10000 postgres:5432
    healthcheck:
      test: [ "CMD", "nc", "-z", "hue", "8888" ]
      timeout: 45s
      interval: 10s
      retries: 10
